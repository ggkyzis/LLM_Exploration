{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download Dependencies"
      ],
      "metadata": {
        "id": "ePkWu-IkU7P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTSp0OawRPKT",
        "outputId": "8ba0953d-c31c-4cfc-d9e5-1680a1d2664c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Model"
      ],
      "metadata": {
        "id": "zr9d9rGiXF-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "# Pipeline Function returns an end-to-end object with preprocessing - model - postprocessing\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0rKYkTYOQfh",
        "outputId": "9d7e129f-761f-466a-b829-cd70b30b2780"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\"   : \"system\",\n",
        "     \"content\": \"You are a helpful assistant!\"},\n",
        "    {\"role\"   : \"user\",\n",
        "     \"content\": \"\"\"What is the meaning of life?\"\"\"},\n",
        "]\n",
        "\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages, # the chat history so far\n",
        "        tokenize=False, # output to be a string\n",
        "        add_generation_prompt=True # helpful to generate a response from the model\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"][len(prompt):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCTB37skOSoS",
        "outputId": "c5726cfb-7cd8-4366-89c2-b9de55f4b02d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a profound and complex question! The meaning of life is a topic that has puzzled philosophers, scientists, and thinkers for centuries. While there is no one definitive answer, I can provide some insights and perspectives that might help you find your own meaning.\n",
            "\n",
            "The concept of the meaning of life is often linked to questions about the purpose, significance, and value of human existence. Some people believe that the meaning of life is to:\n",
            "\n",
            "1. Pursue happiness and fulfillment: Many individuals believe that the meaning of life is to find happiness, contentment, and a sense of fulfillment. This can be achieved through personal relationships, accomplishments, and experiences.\n",
            "2. Discover one's purpose: Others believe that the meaning of life is to discover one's unique purpose or calling. This might involve finding a passion, pursuing a career, or contributing to the greater good.\n",
            "3. Connect with others: The meaning of life can also be found in our connections with others. Building meaningful relationships, helping others, and contributing to the well-being of our communities can give life significance.\n",
            "4. Leave a lasting legacy: Some people believe that the meaning of life is to leave a lasting legacy, whether through creative works, scientific discoveries, or contributions to society.\n",
            "5. Find inner peace: Finally, the meaning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Conversational Bot inside Collab"
      ],
      "metadata": {
        "id": "5Y9DP9wkUEJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "messages = []\n",
        "\n",
        "def add_text(history, text):\n",
        "    global messages  #message[list] is defined globally\n",
        "    history = history + [(text,'')]\n",
        "    messages = messages + [{\"role\":'user', 'content': text}]\n",
        "    return history, ''\n",
        "\n",
        "def generate(history):\n",
        "  global messages\n",
        "  prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "  terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "  outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "  response_msg = outputs[0][\"generated_text\"][len(prompt):]\n",
        "  for char in response_msg:\n",
        "      history[-1][1] += char\n",
        "      yield history\n",
        "  pass\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(value=[], elem_id=\"chatbot\")\n",
        "    with gr.Row():\n",
        "            txt = gr.Textbox(\n",
        "                show_label=False,\n",
        "                placeholder=\"Enter text and press enter\",\n",
        "            )\n",
        "\n",
        "    txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n",
        "            generate, inputs =[chatbot,],outputs = chatbot,)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "6VS90kh1OWjM",
        "outputId": "081ac7dc-7b28-4eb2-e399-110b348d1f93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://72784a7d6b6b39bb74.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://72784a7d6b6b39bb74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://72784a7d6b6b39bb74.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}